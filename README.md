# BC Cold Case Hybrid RAG System - FILAMENT

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)

> **F**orensic **I**ntelligence **L**inking **A**nd **M**atching via **E**mbedded **N**etwork **T**echnology

A Hybrid Retrieval-Augmented Generation (RAG) system designed to connect "hard facts" (structured data) with "loose threads" (unstructured narrative) for cold case investigation in British Columbia.

## ğŸ¯ Mission

Bridge the gap between verified forensic data and unstructured investigative narratives to identify potential matches between **unidentified human remains** and **missing persons** cases in BC.

## ğŸ—ï¸ Architecture Overview

```mermaid
flowchart TB
    subgraph Sources["Phase 1: Data Sources"]
        BCCS[BC Coroners Service]
        NCMPUR[Canada's Missing]
        GIS[BC Open Data GIS]
        Legal[CanLII & BC Courts]
    end
    
    subgraph Processing["Phase 2: Analysis"]
        Extract[Structured Extraction<br/>Unstract/LlamaIndex]
        Graph[Knowledge Graph<br/>Neo4j/FalkorDB]
        Vector[Vector Search<br/>pgvector]
    end
    
    subgraph Intelligence["Phase 3: Bioinformatics"]
        Phenotype[Phenotype Recommender]
        Isotope[Isotope Analysis]
    end
    
    subgraph Output["Results"]
        Matches[Potential Matches]
        Viz[Kepler.gl Visualization]
    end
    
    Sources --> Extract
    Extract --> Graph
    Extract --> Vector
    Graph --> Matches
    Vector --> Matches
    Intelligence --> Matches
    Matches --> Viz
```

## ğŸ“ Project Structure

```
filament/
â”œâ”€â”€ docs/                    # Detailed documentation
â”‚   â”œâ”€â”€ architecture.md      # System architecture
â”‚   â”œâ”€â”€ data_sources.md      # Phase 1: Data sources
â”‚   â”œâ”€â”€ analysis_approaches.md # Phase 2: Analysis
â”‚   â”œâ”€â”€ bioinformatics.md    # Phase 3: Bio-analysis
â”‚   â””â”€â”€ tech_stack.md        # Technology stack
â”œâ”€â”€ code/                    # Source code
â”‚   â”œâ”€â”€ extraction/          # Structured data extraction
â”‚   â”œâ”€â”€ graph/               # Knowledge graph operations
â”‚   â”œâ”€â”€ search/              # Vector search & matching
â”‚   â””â”€â”€ scrapers/            # Data source scrapers
â”œâ”€â”€ data/                    # Data directory (gitignored)
â”‚   â”œâ”€â”€ raw/                 # Raw data files
â”‚   â”œâ”€â”€ processed/           # Processed datasets
â”‚   â””â”€â”€ external/            # External reference data
â”œâ”€â”€ config/                  # Configuration files
â”œâ”€â”€ notebooks/               # Jupyter notebooks
â””â”€â”€ tests/                   # Test suite
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- PostgreSQL with pgvector extension
- Ollama (for local LLM inference)

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/filament.git
cd filament

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/macOS

# Install dependencies
pip install -r requirements.txt

# Copy environment template
cp .env.example .env
# Edit .env with your configuration
```

### Running the System

```bash
# Start PostgreSQL with pgvector
# (See docs/tech_stack.md for setup instructions)

# Initialize the database
python -m code.db.init

# Start the extraction pipeline
python -m code.extraction.pipeline
```

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| [Architecture](docs/architecture.md) | System design and data flow |
| [Data Sources](docs/data_sources.md) | BC-specific data sources and integration |
| [Analysis Approaches](docs/analysis_approaches.md) | Extraction, graph, and vector search |
| [Bioinformatics](docs/bioinformatics.md) | Phenotype and isotope analysis |
| [Tech Stack](docs/tech_stack.md) | Technology choices and setup |

## ğŸ”’ Privacy & Ethics

This system is designed with privacy as a core principle:

- **No raw DNA data**: Only phenotypic descriptions and metadata are processed
- **Local LLM inference**: Sensitive data never leaves the system
- **Public sources only**: All data comes from publicly accessible sources
- **Audit trail**: All matches and inferences are logged for review

## ğŸ¤ Contributing

See [docs/contributing.md](docs/contributing.md) for guidelines on how to contribute to this project.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

**Disclaimer**: This is a research tool intended to assist investigators. All potential matches must be verified through proper forensic and legal channels before any action is taken.
